{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Convert HTML documents to text\n",
    "\n",
    "def convert_to_text(src_dir, target_dir):\n",
    "    for file in os.listdir('../data/original_policies'):\n",
    "        with open('../data/original_policies' + '/' + file, 'r', encoding=\"ISO-8859-1\") as f:\n",
    "            print(file)\n",
    "            data = f.read()\n",
    "            # print(data)\n",
    "            bs = BeautifulSoup(data,'html.parser')\n",
    "            texts = bs.findAll(['title', 'body','p','strong'])\n",
    "\n",
    "        with open('../data/clean_policies' + '/' + file, 'w') as f:\n",
    "            for t in texts:\n",
    "                f.write(t.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Remove tags from the documents\n",
    "def convert_clean_summaries(src_dir, target_dir):\n",
    "    for file in os.listdir('../data/sanitized_policies'):\n",
    "        with open('../data/sanitized_policies' + '/' + file, 'r', encoding=\"ISO-8859-1\") as f:\n",
    "\n",
    "            cleanr = re.compile('<.*?>')\n",
    "            cleantext = re.sub(cleanr, '', f.read())\n",
    "            filename = file.split('.', -1)[0] + '.txt'\n",
    "\n",
    "        with open('../data/notags_policies' + '/' + filename, 'w') as f:\n",
    "            f.write(cleantext)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# remove all the punctuations from the text\n",
    "def remove_punctuation(data):\n",
    "    data = re.sub(\"_\", \"\", data)\n",
    "    data = re.sub(\"[^\\w\\s]\", \"\", data)\n",
    "    data = re.sub(' +', ' ', data)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import os\n",
    "import numpy as np\n",
    "import scipy.spatial.distance as distance\n",
    "import gensim\n",
    "from collections import defaultdict\n",
    "import re\n",
    "from src.preprocess import remove_punctuation\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import math\n",
    "\n",
    "\n",
    "word2vec = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "    \"../data/GoogleNews-vectors-negative300.bin\", binary=True)\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "para2vec = gensim.models.KeyedVectors.load_doc2vec_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "CLASS_NUM = {'Data Retention' : 0,\n",
    " 'Data Security' : 1,\n",
    " 'Do Not Track' : 2,\n",
    " 'First Party Collection/Use' : 3,\n",
    " 'International and Specific Audiences' : 4,\n",
    " 'Policy Change' : 5,\n",
    " 'Third Party Sharing/Collection' : 6,\n",
    " 'User Access, Edit and Deletion' : 7,\n",
    " 'User Choice/Control' : 8,\n",
    " 'Other' : 9}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def word_weights(filepath):\n",
    "\n",
    "    with open(filepath) as f:\n",
    "        content = f.read()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def get_sentence_vector(sentence):\n",
    "    print(\"Geting maximum sentence similarity of each sentence...\")\n",
    "    word2vec_dict = word2vec.vocab.keys()\n",
    "\n",
    "    wordvecs = []\n",
    "    # sentence = nlp(sentence)\n",
    "    for word in sentence:\n",
    "        # Words in a summary that are not covered by Word2Vec are discarded.\n",
    "        word = str(word).lower()\n",
    "        if word in word2vec_dict:\n",
    "            wordvecs.append(word2vec[word])\n",
    "\n",
    "    wordvecs = np.array(wordvecs)\n",
    "    # represent each sentence as average of its word embedding\n",
    "    sentence_score = np.mean(wordvecs, axis=0)\n",
    "\n",
    "\n",
    "    return sentence_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Modify explanation\n",
    "\n",
    "\n",
    "#  Paragraph2Vec technique includes several different algorithm. \n",
    "# The only difference from word2vec is inclusion of documents along with words as input nodes. \n",
    "# P2V neural net has input nodes representing documents in the training data (see fig 2).\n",
    "# The rationale behind including documents as input nodes is based upon considering documents as another context. \n",
    "# In this abstract sense of context there is no difference between a word and a document. \n",
    "# At the time of training we consider (context set, target) pairs as in word2vec, however,\n",
    "# for P2V document is also considered a member of the context set.\n",
    "# The objective function and the training update steps are exactly the same as word2vec.\n",
    "\n",
    "def get_paragraph_vector(paragraph):\n",
    "#     print(\"Vectorizing Paragraph\")\n",
    "    word2vec_dict = word2vec.vocab.keys()\n",
    "    sentences = nlp(paragraph)\n",
    "\n",
    "    wordvecs = []\n",
    "    # sentence = nlp(sentence)\n",
    "    for word in sentences:\n",
    "        # Words in a summary that are not covered by Word2Vec are discarded.\n",
    "        word = str(word.text).lower()\n",
    "        if word in word2vec_dict:\n",
    "            wordvecs.append(word2vec[word])\n",
    "\n",
    "    wordvecs = np.array(wordvecs)\n",
    "    # represent each sentence as average of its word embedding\n",
    "    paragraph_score = np.mean(wordvecs, axis=0)\n",
    "\n",
    "\n",
    "    return paragraph_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def calculate_sentence_sim(s1):\n",
    "    #       - Statement 1 (Clear Purpose): For what purposes does the company use personal information?\n",
    "    #     # - Statement 2 (Third Parties): Does the company share my information with third parties?\n",
    "    #     # - Statement 3 (Limited Collection): Does the company combine my information with data from other sources?\n",
    "    #     # - Statement 4 (Limited Use): Will the company sell, re-package or commercialize my data?\n",
    "    #     # - Statement 5 (Retention): Will the company retain my data? What is their retention policy?\n",
    "\n",
    "    s  = get_sentence_vector(s1)\n",
    "    c1 = get_sentence_vector(nlp(remove_punctuation(\"For what purposes does the company use personal information?\")))\n",
    "    c2 = get_sentence_vector(nlp(remove_punctuation(\"Does the company share my information with third parties?\")))\n",
    "    c3 = get_sentence_vector(nlp(remove_punctuation(\"Does the company combine my information with data from other sources?\")))\n",
    "    c4 = get_sentence_vector(nlp(remove_punctuation(\"Will the company sell, re-package or commercialize my data?\")))\n",
    "    c5 = get_sentence_vector(nlp(remove_punctuation(\"Will the company retain my data? What is their retention policy?\")))\n",
    "\n",
    "    res = [0] * 5\n",
    "    res[0] = 1 - distance.cosine(s, c1)\n",
    "    res[1] = 1 - distance.cosine(s, c2)\n",
    "    res[2] = 1 - distance.cosine(s, c3)\n",
    "    res[3] = 1 - distance.cosine(s, c4)\n",
    "    res[4] = 1 - distance.cosine(s, c5)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNeighborsClassifier  Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "## Get Train Data\n",
    "import json\n",
    "\n",
    "with open('../data/notags_policies/sample_train/parsed_policies.txt') as f:\n",
    "    all_policies = json.loads(f.read())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def get_vector_for_policy(data, training_files):\n",
    "    para_dict = {}\n",
    "    \n",
    "    for filename in training_files:\n",
    "        for key in data[filename]:\n",
    "            para_dict[key] = CLASS_NUM[data[filename][key]]\n",
    "\n",
    "    return para_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def get_k_fold_cross_validation(classifier, X, Y, K):\n",
    "    res = []\n",
    "    scores = []\n",
    "    for k in range(K):\n",
    "        train_x = [x for i, x in enumerate(X) if i % K != k]\n",
    "        train_y = [y for i, y in enumerate(Y) if i % K != k]\n",
    "\n",
    "        validate_x = [x for i, x in enumerate(X) if i % K == k]\n",
    "        validate_y = [y for i, y in enumerate(Y) if i % K == k]\n",
    "        res.append(((train_x, train_y), (validate_x, validate_y)))\n",
    "\n",
    "    for train, validate in res:\n",
    "        classifier.fit(train[0], train[1])\n",
    "        score = classifier.score(validate[0], validate[1])\n",
    "        scores.append(score)\n",
    "\n",
    "    return np.array(scores).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "with open('../data/notags_policies/sample_train/train_list.txt') as f:\n",
    "    content = f.read()\n",
    "content\n",
    "\n",
    "training_files = [\"414_washingtonian.com.csv\", \"856_sciencemag.org.csv\", \"70_meredith.com.csv\", \"1636_sidearmsports.com.csv\", \"1224_austincc.edu.csv\", \"1510_jibjab.com.csv\", \"453_barnesandnoble.com.csv\", \"1099_enthusiastnetwork.com.csv\", \"98_neworleansonline.com.csv\", \"59_liquor.com.csv\", \"940_internetbrands.com.csv\", \"883_ted.com.csv\", \"82_sheknows.com.csv\", \"394_newsbusters.org.csv\", \"164_adweek.com.csv\", \"33_nbcuniversal.com.csv\", \"640_gamestop.com.csv\", \"652_randomhouse.com.csv\", \"1618_sltrib.com.csv\", \"1713_latinpost.com.csv\", \"891_everydayhealth.com.csv\", \"105_amazon.com.csv\", \"303_reddit.com.csv\", \"58_esquire.com.csv\", \"591_google.com.csv\", \"207_reference.com.csv\", \"32_voxmedia.com.csv\", \"1050_honda.com.csv\", \"144_style.com.csv\", \"807_lodgemfg.com.csv\", \"1034_aol.com.csv\", \"1089_freep.com.csv\", \"1164_acbj.com.csv\", \"517_kaleidahealth.org.csv\", \"1694_lids.com.csv\", \"1028_redorbit.com.csv\", \"1419_miaminewtimes.com.csv\", \"1468_rockstargames.com.csv\", \"1683_dailynews.com.csv\", \"746_kraftrecipes.com.csv\", \"348_walmart.com.csv\", \"928_stlouisfed.org.csv\", \"21_imdb.com.csv\", \"320_timeinc.com.csv\", \"20_theatlantic.com.csv\", \"202_foodallergy.org.csv\", \"26_nytimes.com.csv\", \"1666_wsmv.com.csv\", \"1070_wnep.com.csv\", \"686_military.com.csv\", \"1539_geocaching.com.csv\", \"641_cbsinteractive.com.csv\", \"200_washingtonpost.com.csv\", \"135_instagram.com.csv\", \"1360_thehill.com.csv\", \"1306_chasepaymentech.com.csv\", \"1470_steampowered.com.csv\", \"186_abcnews.com.csv\", \"1610_post-gazette.com.csv\", \"1708_foxsports.com.csv\", \"635_playstation.com.csv\", \"175_mlb.mlb.com.csv\", \"541_ifsa-butler.org.csv\", \"359_vikings.com.csv\", \"1259_fool.com.csv\", \"133_fortune.com.csv\", \"1300_bankofamerica.com.csv\", \"962_lynda.com.csv\", \"1106_allstate.com.csv\", \"1582_msn.com.csv\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "training_data = get_vector_for_policy(all_policies, training_files)\n",
    "t_data = {}\n",
    "for i in training_data:\n",
    "    if not np.isnan(get_paragraph_vector(i)).any():\n",
    "        t_data[i] = training_data[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "len(t_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def train_single_file(train_data):\n",
    "    x_data = []\n",
    "    for i in train_data.keys():\n",
    "        x_data.append(get_paragraph_vector(i))\n",
    "\n",
    "    y_data = list(train_data.values())\n",
    "    \n",
    "    \n",
    "    \n",
    "    scores = []\n",
    "    knns = []\n",
    "    for i in range(1, 10):\n",
    "        knn = KNeighborsClassifier(n_neighbors = i)\n",
    "        mean_score = get_k_fold_cross_validation(knn, x_data, y_data, 5)\n",
    "        scores.append(mean_score)\n",
    "        knns.append(knn)\n",
    "\n",
    "    index = np.argmax(scores)\n",
    "    \n",
    "    print(\"Grid search result:\")\n",
    "    print(scores[index])\n",
    "    print(scores)\n",
    "    return knns[index], x_data, y_data \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "knn_gscv, x_data, y_data = train_single_file(t_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "para_test_dict = {}\n",
    "\n",
    "for filename in all_policies.keys():\n",
    "    if filename not in training_files:\n",
    "        for key in all_policies[filename]:\n",
    "            para_test_dict[key] = CLASS_NUM[all_policies[filename][key]]\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "y_predict = []\n",
    "\n",
    "for i in para_test_dict.keys():\n",
    "\n",
    "    if np.isnan(get_paragraph_vector(i)).any():\n",
    "        y_predict.append(9)\n",
    "    else:\n",
    "        y_predict.append(knn_gscv.predict([get_paragraph_vector(i)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "y_knn_predict = []\n",
    "for i in y_predict:\n",
    "    if isinstance(i, int):\n",
    "        y_knn_predict.append(i)    \n",
    "    else:\n",
    "        y_knn_predict.append(i[0])\n",
    "        \n",
    "y_knn_predict\n",
    "len(y_knn_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "data = [x_data, y_data ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLPClassifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "def optimize_parameters(data, noOfFirstHiddenLayer, maxIter, learningRateInit):\n",
    "    nf = noOfFirstHiddenLayer\n",
    "    mi = maxIter\n",
    "    lr = learningRateInit\n",
    "    params = [[x, y, 10 ** z]\n",
    "              for x in range(nf[0], nf[1] + nf[2], nf[2])\n",
    "              for y in range(mi[0], mi[1] + mi[2], mi[2])\n",
    "              for z in range(int(np.log10(lr[0])),\n",
    "                             int(np.log10(lr[1])) + lr[2],\n",
    "                             lr[2])]\n",
    "    scores = []\n",
    "    mlps = []\n",
    "    for param in params:\n",
    "        mlp = MLPClassifier((param[0], 10), max_iter=param[1],\n",
    "                            learning_rate_init=param[2])\n",
    "        mean_score = get_k_fold_cross_validation(mlp, data[0], data[1], 2)\n",
    "\n",
    "\n",
    "        scores.append(mean_score)\n",
    "        mlps.append(mlp)\n",
    "\n",
    "    index = np.argmax(scores)\n",
    "\n",
    "    print(\"Grid search result:\")\n",
    "\n",
    "    print(params[index])\n",
    "    print(scores[index])\n",
    "    return mlps[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "mlp_model = optimize_parameters(data, [100,130, 10],[200, 400, 100], [0.001, 0.1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "y_m_predict = []\n",
    "\n",
    "for i in para_test_dict.keys():\n",
    "\n",
    "    if np.isnan(get_paragraph_vector(i)).any():\n",
    "        y_m_predict.append(9)\n",
    "    else:\n",
    "        y_m_predict.append(mlp_model.predict([get_paragraph_vector(i)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "y_mlp_predict = []\n",
    "for i in y_m_predict:\n",
    "    if isinstance(i, int):\n",
    "        y_mlp_predict.append(i)    \n",
    "    else:\n",
    "        y_mlp_predict.append(i[0])\n",
    "        \n",
    "\n",
    "len(y_mlp_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "clf = LDA()\n",
    "mean_score = get_k_fold_cross_validation(clf, x_data, y_data, 5)\n",
    "\n",
    "mean_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "y_ld_predict = []\n",
    "\n",
    "for i in para_test_dict.keys():\n",
    "\n",
    "    if np.isnan(get_paragraph_vector(i)).any():\n",
    "        y_ld_predict.append(9)\n",
    "    else:\n",
    "        y_ld_predict.append(clf.predict([get_paragraph_vector(i)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "y_lda_predict = []\n",
    "for i in y_ld_predict:\n",
    "    if isinstance(i, int):\n",
    "        y_lda_predict.append(i)    \n",
    "    else:\n",
    "        y_lda_predict.append(i[0])\n",
    "        \n",
    "\n",
    "len(y_lda_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **********"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def get_policy_vectors(filepath):\n",
    "\n",
    "    with open(filepath,'r') as f:\n",
    "\n",
    "        data = nlp(remove_punctuation(f.read()))\n",
    "        res = {}\n",
    "        for sentence in data.sents:\n",
    "            sentence_sim_vector = calculate_sentence_sim(sentence)\n",
    "            # if max(sentence_sim_vector) >= 0.65:\n",
    "            #     print(sentence)\n",
    "            res[sentence] = sentence_sim_vector\n",
    "\n",
    "    return res\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def get_policy_vectors_sents(filepath):\n",
    "\n",
    "    with open(filepath,'r') as f:\n",
    "\n",
    "        data = nlp(remove_punctuation(f.read()))\n",
    "        res = {}\n",
    "        for sentence in data.sents:\n",
    "\n",
    "            sentence_sim_vector = get_sentence_vector(nlp((str(sentence.text))))\n",
    "\n",
    "            if np.isnan(sentence_sim_vector).any():\n",
    "                res[sentence] = np.array([0.0]*300)\n",
    "                continue\n",
    "            print(type(sentence_sim_vector))\n",
    "            # if math.isnan(sentence_sim_vector):\n",
    "            #     continue\n",
    "            # if max(sentence_sim_vector) >= 0.65:\n",
    "            #     print(sentence)\n",
    "            res[sentence] = sentence_sim_vector\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# def get_all_policy_vectors(src_path):\n",
    "#     all_policies = {}\n",
    "#     for file in os.listdir(src_path):\n",
    "#         with open(src_path + '/'+ file, 'r') as f:\n",
    "#             all_policies[file] = get_all_policy_vectors(src_path + '/'+ file)\n",
    "#\n",
    "#     return all_policies\n",
    "\n",
    "\n",
    "\n",
    "similarity_array = get_policy_vectors('../data/notags_policies/33_nbcuniversal.txt')\n",
    "vector_array = get_policy_vectors_sents('../data/notags_policies/33_nbcuniversal.txt')\n",
    "\n",
    "class_array= {}\n",
    "for k,row in similarity_array.items():\n",
    "    if max(row) >= 0.65:\n",
    "        class_array[k] = row.index(max(row)) + 1\n",
    "    else:\n",
    "        class_array[k] = 0\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# CLASS_NUM = {'Data Retention' : 0,\n",
    "#  'Data Security' : 1,\n",
    "#  'Do Not Track' : 2,\n",
    "#  'First Party Collection/Use' : 3,\n",
    "#  'International and Specific Audiences' : 4,\n",
    "#  'Policy Change' : 5,\n",
    "#  'Third Party Sharing/Collection' : 6,\n",
    "#  'User Access, Edit and Deletion' : 7,\n",
    "#  'User Choice/Control' : 8,\n",
    "#  'Other' : 9}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results: Knearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "\n",
    "predicted = y_knn_predict\n",
    "y_test =list(para_test_dict.values())\n",
    "\n",
    "\n",
    "\n",
    "precision_knn, recall_knn, fscore_knn, support_knn = score(y_test, predicted)\n",
    "\n",
    "print('precision: {}'.format(precision))\n",
    "print('recall: {}'.format(recall))\n",
    "print('fscore: {}'.format(fscore))\n",
    "print('support: {}'.format(support))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results: MLP Classifier 2 Hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "\n",
    "predicted = y_mlp_predict\n",
    "y_test =list(para_test_dict.values())\n",
    "\n",
    "\n",
    "\n",
    "precision_mlp, recall_mlp, fscore_mlp, support_mlp = score(y_test, predicted)\n",
    "\n",
    "print('precision: {}'.format(precision))\n",
    "print('recall: {}'.format(recall))\n",
    "print('fscore: {}'.format(fscore))\n",
    "print('support: {}'.format(support))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results: MLP Classifier 2 Hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "\n",
    "predicted = y_lda_predict\n",
    "y_test =list(para_test_dict.values())\n",
    "\n",
    "\n",
    "\n",
    "precision_lda, recall_lda, fscore_lda, support_lda = score(y_test, predicted)\n",
    "\n",
    "print('precision: {}'.format(precision))\n",
    "print('recall: {}'.format(recall))\n",
    "print('fscore: {}'.format(fscore))\n",
    "print('support: {}'.format(support))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "plt.title(\"Precision by different models\")\n",
    "plt.scatter(range(0,10), precision_knn, color='blue', label= 'Knn')\n",
    "plt.scatter(range(0,10), precision_mlp, color='orange', label = 'MLP' )\n",
    "plt.scatter(range(0,10), precision_lda, color='green', label=\"LDA\")\n",
    "\n",
    "plt.xticks([0,1, 2,3,4,5,6,7,8,9],('Data Retention','Data Security', 'Do Not Track', 'First Party Collection/Use', 'International and Specific Audiences',\n",
    "            'Policy Change','Third Party Sharing/Collection','User Access, Edit and Deletion','User Choice/Control','Other'), rotation=90)\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "plt.title(\"Fscore by different models\")\n",
    "plt.scatter(range(0,10), fscore_knn, color='blue', label= 'Knn')\n",
    "plt.scatter(range(0,10), fscore_mlp, color='orange', label = 'MLP' )\n",
    "plt.scatter(range(0,10), fscore_lda, color='green', label=\"LDA\")\n",
    "\n",
    "plt.xticks([0,1, 2,3,4,5,6,7,8,9],('Data Retention','Data Security', 'Do Not Track', 'First Party Collection/Use', 'International and Specific Audiences',\n",
    "            'Policy Change','Third Party Sharing/Collection','User Access, Edit and Deletion','User Choice/Control','Other'), rotation=90)\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}