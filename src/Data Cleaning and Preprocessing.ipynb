{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exract content from HTML documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def convert_to_text(src_dir, target_dir):\n",
    "    for file in os.listdir('../data/original_policies'):\n",
    "        with open('../data/original_policies' + '/' + file, 'r', encoding=\"ISO-8859-1\") as f:\n",
    "            print(file)\n",
    "            data = f.read()\n",
    "            # print(data)\n",
    "            bs = BeautifulSoup(data,'html.parser')\n",
    "            texts = bs.findAll(['title', 'body','p','strong'])\n",
    "\n",
    "        with open('../data/clean_policies' + '/' + file, 'w') as f:\n",
    "            for t in texts:\n",
    "                f.write(t.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove tags from the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_clean_summaries(src_dir, target_dir):\n",
    "    for file in os.listdir('../data/sanitized_policies'):\n",
    "        with open('../data/sanitized_policies' + '/' + file, 'r', encoding=\"ISO-8859-1\") as f:\n",
    "\n",
    "            cleanr = re.compile('<.*?>')\n",
    "            cleantext = re.sub(cleanr, '', f.read())\n",
    "            filename = file.split('.', -1)[0] + '.txt'\n",
    "\n",
    "        with open('../data/notags_policies' + '/' + filename, 'w') as f:\n",
    "            f.write(cleantext)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Punctions from data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all the punctuations from the text\n",
    "def remove_punctuation(data):\n",
    "    data = re.sub(\"_\", \"\", data)\n",
    "    data = re.sub(\"[^\\w\\s]\", \"\", data)\n",
    "    data = re.sub(' +', ' ', data)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Insert Code for creating training dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify Topics for each category of Privacy Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('../data/notags_policies/preprocessed_train/parsed_policies.txt') as f :\n",
    "    labeled_data = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_topics(data):\n",
    "    \n",
    "    labeled_sentences = {}\n",
    "    \n",
    "    # Create dictionary with sentences and category:\n",
    "    for key in data:\n",
    "        for sentences in data[key]:\n",
    "            labeled_sentences[sentences] = data[key][sentences]  \n",
    "    \n",
    "    # Regroup dictionary to merge sentences according to category\n",
    "\n",
    "    category_sentences = defaultdict(list)\n",
    "\n",
    "    for key, value in sorted(labeled_sentences.items()):\n",
    "        category_sentences[value].append(key)\n",
    "    \n",
    "    \n",
    "    most_common_topics = get_common_topics(category_sentences)\n",
    "    \n",
    "    with open('../results/policy_topics.txt', 'a') as file:\n",
    "        \n",
    "        for key, value in most_common_topics.items():\n",
    "            file.write('Most common topics for category ' + str(key) + '\\n')\n",
    "            file.write(str([word for word,_ in value]) + '\\n\\n')\n",
    "    return most_common_topics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def get_common_topics(category_dict):\n",
    "    all_topics = {}\n",
    "    for category in category_dict:\n",
    "        bag_of_words = {}\n",
    "        doc = nlp(remove_punctuation(' '.join(category_dict[category])))\n",
    "        for token in doc:\n",
    "            if not token.is_stop:\n",
    "                if token.text in bag_of_words:\n",
    "                    bag_of_words[token.text] += 1\n",
    "                else:\n",
    "                    bag_of_words[token.text] = 1\n",
    "        top_topics = Counter(bag_of_words).most_common(30)\n",
    "        all_topics[category] = top_topics\n",
    "    return all_topics\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/notags_policies/preprocessed_train/topics.txt','w') as f:\n",
    "    f.write(json.dumps(identify_topics(labeled_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
