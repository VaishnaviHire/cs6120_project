{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Convert HTML documents to text\n",
    "\n",
    "def convert_to_text(src_dir, target_dir):\n",
    "    for file in os.listdir('../data/original_policies'):\n",
    "        with open('../data/original_policies' + '/' + file, 'r', encoding=\"ISO-8859-1\") as f:\n",
    "            print(file)\n",
    "            data = f.read()\n",
    "            # print(data)\n",
    "            bs = BeautifulSoup(data,'html.parser')\n",
    "            texts = bs.findAll(['title', 'body','p','strong'])\n",
    "\n",
    "        with open('../data/clean_policies' + '/' + file, 'w') as f:\n",
    "            for t in texts:\n",
    "                f.write(t.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Remove tags from the documents\n",
    "def convert_clean_summaries(src_dir, target_dir):\n",
    "    for file in os.listdir('../data/sanitized_policies'):\n",
    "        with open('../data/sanitized_policies' + '/' + file, 'r', encoding=\"ISO-8859-1\") as f:\n",
    "\n",
    "            cleanr = re.compile('<.*?>')\n",
    "            cleantext = re.sub(cleanr, '', f.read())\n",
    "            filename = file.split('.', -1)[0] + '.txt'\n",
    "\n",
    "        with open('../data/notags_policies' + '/' + filename, 'w') as f:\n",
    "            f.write(cleantext)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# remove all the punctuations from the text\n",
    "def remove_punctuation(data):\n",
    "    data = re.sub(\"_\", \"\", data)\n",
    "    data = re.sub(\"[^\\w\\s]\", \"\", data)\n",
    "    data = re.sub(' +', ' ', data)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/GoogleNews-vectors-negative300.bin'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-18fc69a513f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m word2vec = gensim.models.KeyedVectors.load_word2vec_format(\n\u001b[0;32m---> 14\u001b[0;31m     \"../data/GoogleNews-vectors-negative300.bin\", binary=True)\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'en_core_web_sm'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/DS2000_grading/venv/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[1;32m   1496\u001b[0m         return _load_word2vec_format(\n\u001b[1;32m   1497\u001b[0m             \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1498\u001b[0;31m             limit=limit, datatype=datatype)\n\u001b[0m\u001b[1;32m   1499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_keras_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/DS2000_grading/venv/lib/python3.7/site-packages/gensim/models/utils_any2vec.py\u001b[0m in \u001b[0;36m_load_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loading projection weights from %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m         \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_unicode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m         \u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# throws for invalid file format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/DS2000_grading/venv/lib/python3.7/site-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(uri, mode, buffering, encoding, errors, newline, closefd, opener, ignore_ext, transport_params)\u001b[0m\n\u001b[1;32m    306\u001b[0m         \u001b[0mbuffering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m         \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    309\u001b[0m     )\n\u001b[1;32m    310\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/DS2000_grading/venv/lib/python3.7/site-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36m_shortcut_open\u001b[0;34m(uri, mode, ignore_ext, buffering, encoding, errors)\u001b[0m\n\u001b[1;32m    515\u001b[0m     \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPY3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_builtin_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_uri\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muri_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mopen_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mopen_kwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_builtin_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_uri\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muri_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/GoogleNews-vectors-negative300.bin'"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import os\n",
    "import numpy as np\n",
    "import scipy.spatial.distance as distance\n",
    "import gensim\n",
    "from collections import defaultdict\n",
    "import re\n",
    "from src.preprocess import remove_punctuation\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import math\n",
    "\n",
    "\n",
    "word2vec = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "    \"../data/GoogleNews-vectors-negative300.bin\", binary=True)\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "para2vec = gensim.models.KeyedVectors.load_doc2vec_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "CLASS_NUM = {'Data Retention' : 0,\n",
    " 'Data Security' : 1,\n",
    " 'Do Not Track' : 2,\n",
    " 'First Party Collection/Use' : 3,\n",
    " 'International and Specific Audiences' : 4,\n",
    " 'Policy Change' : 5,\n",
    " 'Third Party Sharing/Collection' : 6,\n",
    " 'User Access, Edit and Deletion' : 7,\n",
    " 'User Choice/Control' : 8,\n",
    " 'Other' : 9}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def word_weights(filepath):\n",
    "\n",
    "    with open(filepath) as f:\n",
    "        content = f.read()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def get_sentence_vector(sentence):\n",
    "    print(\"Geting maximum sentence similarity of each sentence...\")\n",
    "    word2vec_dict = word2vec.vocab.keys()\n",
    "\n",
    "    wordvecs = []\n",
    "    # sentence = nlp(sentence)\n",
    "    for word in sentence:\n",
    "        # Words in a summary that are not covered by Word2Vec are discarded.\n",
    "        word = str(word).lower()\n",
    "        if word in word2vec_dict:\n",
    "            wordvecs.append(word2vec[word])\n",
    "\n",
    "    wordvecs = np.array(wordvecs)\n",
    "    # represent each sentence as average of its word embedding\n",
    "    sentence_score = np.mean(wordvecs, axis=0)\n",
    "\n",
    "\n",
    "    return sentence_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Modify explanation\n",
    "\n",
    "\n",
    "#  Paragraph2Vec technique includes several different algorithm. \n",
    "# The only difference from word2vec is inclusion of documents along with words as input nodes. \n",
    "# P2V neural net has input nodes representing documents in the training data (see fig 2).\n",
    "# The rationale behind including documents as input nodes is based upon considering documents as another context. \n",
    "# In this abstract sense of context there is no difference between a word and a document. \n",
    "# At the time of training we consider (context set, target) pairs as in word2vec, however,\n",
    "# for P2V document is also considered a member of the context set.\n",
    "# The objective function and the training update steps are exactly the same as word2vec.\n",
    "\n",
    "def get_paragraph_vector(paragraph):\n",
    "#     print(\"Vectorizing Paragraph\")\n",
    "    word2vec_dict = word2vec.vocab.keys()\n",
    "    sentences = nlp(paragraph)\n",
    "\n",
    "    wordvecs = []\n",
    "    # sentence = nlp(sentence)\n",
    "    for word in sentences:\n",
    "        # Words in a summary that are not covered by Word2Vec are discarded.\n",
    "        word = str(word.text).lower()\n",
    "        if word in word2vec_dict:\n",
    "            wordvecs.append(word2vec[word])\n",
    "\n",
    "    wordvecs = np.array(wordvecs)\n",
    "    # represent each sentence as average of its word embedding\n",
    "    paragraph_score = np.mean(wordvecs, axis=0)\n",
    "\n",
    "\n",
    "    return paragraph_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def calculate_sentence_sim(s1):\n",
    "    #       - Statement 1 (Clear Purpose): For what purposes does the company use personal information?\n",
    "    #     # - Statement 2 (Third Parties): Does the company share my information with third parties?\n",
    "    #     # - Statement 3 (Limited Collection): Does the company combine my information with data from other sources?\n",
    "    #     # - Statement 4 (Limited Use): Will the company sell, re-package or commercialize my data?\n",
    "    #     # - Statement 5 (Retention): Will the company retain my data? What is their retention policy?\n",
    "\n",
    "    s  = get_sentence_vector(s1)\n",
    "    c1 = get_sentence_vector(nlp(remove_punctuation(\"For what purposes does the company use personal information?\")))\n",
    "    c2 = get_sentence_vector(nlp(remove_punctuation(\"Does the company share my information with third parties?\")))\n",
    "    c3 = get_sentence_vector(nlp(remove_punctuation(\"Does the company combine my information with data from other sources?\")))\n",
    "    c4 = get_sentence_vector(nlp(remove_punctuation(\"Will the company sell, re-package or commercialize my data?\")))\n",
    "    c5 = get_sentence_vector(nlp(remove_punctuation(\"Will the company retain my data? What is their retention policy?\")))\n",
    "\n",
    "    res = [0] * 5\n",
    "    res[0] = 1 - distance.cosine(s, c1)\n",
    "    res[1] = 1 - distance.cosine(s, c2)\n",
    "    res[2] = 1 - distance.cosine(s, c3)\n",
    "    res[3] = 1 - distance.cosine(s, c4)\n",
    "    res[4] = 1 - distance.cosine(s, c5)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNeighborsClassifier  Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "## Get Train Data\n",
    "import json\n",
    "\n",
    "with open('../data/notags_policies/sample_train/parsed_policies.txt') as f:\n",
    "    all_policies = json.loads(f.read())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def get_vector_for_policy(data, training_files):\n",
    "    para_dict = {}\n",
    "    \n",
    "    for filename in training_files:\n",
    "        for key in data[filename]:\n",
    "            para_dict[key] = CLASS_NUM[data[filename][key]]\n",
    "\n",
    "    return para_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def get_k_fold_cross_validation(classifier, X, Y, K):\n",
    "    res = []\n",
    "    scores = []\n",
    "    for k in range(K):\n",
    "        train_x = [x for i, x in enumerate(X) if i % K != k]\n",
    "        train_y = [y for i, y in enumerate(Y) if i % K != k]\n",
    "\n",
    "        validate_x = [x for i, x in enumerate(X) if i % K == k]\n",
    "        validate_y = [y for i, y in enumerate(Y) if i % K == k]\n",
    "        res.append(((train_x, train_y), (validate_x, validate_y)))\n",
    "\n",
    "    for train, validate in res:\n",
    "        classifier.fit(train[0], train[1])\n",
    "        score = classifier.score(validate[0], validate[1])\n",
    "        scores.append(score)\n",
    "\n",
    "    return np.array(scores).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "with open('../data/notags_policies/sample_train/train_list.txt') as f:\n",
    "    content = f.read()\n",
    "content\n",
    "\n",
    "training_files = [\"414_washingtonian.com.csv\", \"856_sciencemag.org.csv\", \"70_meredith.com.csv\", \"1636_sidearmsports.com.csv\", \"1224_austincc.edu.csv\", \"1510_jibjab.com.csv\", \"453_barnesandnoble.com.csv\", \"1099_enthusiastnetwork.com.csv\", \"98_neworleansonline.com.csv\", \"59_liquor.com.csv\", \"940_internetbrands.com.csv\", \"883_ted.com.csv\", \"82_sheknows.com.csv\", \"394_newsbusters.org.csv\", \"164_adweek.com.csv\", \"33_nbcuniversal.com.csv\", \"640_gamestop.com.csv\", \"652_randomhouse.com.csv\", \"1618_sltrib.com.csv\", \"1713_latinpost.com.csv\", \"891_everydayhealth.com.csv\", \"105_amazon.com.csv\", \"303_reddit.com.csv\", \"58_esquire.com.csv\", \"591_google.com.csv\", \"207_reference.com.csv\", \"32_voxmedia.com.csv\", \"1050_honda.com.csv\", \"144_style.com.csv\", \"807_lodgemfg.com.csv\", \"1034_aol.com.csv\", \"1089_freep.com.csv\", \"1164_acbj.com.csv\", \"517_kaleidahealth.org.csv\", \"1694_lids.com.csv\", \"1028_redorbit.com.csv\", \"1419_miaminewtimes.com.csv\", \"1468_rockstargames.com.csv\", \"1683_dailynews.com.csv\", \"746_kraftrecipes.com.csv\", \"348_walmart.com.csv\", \"928_stlouisfed.org.csv\", \"21_imdb.com.csv\", \"320_timeinc.com.csv\", \"20_theatlantic.com.csv\", \"202_foodallergy.org.csv\", \"26_nytimes.com.csv\", \"1666_wsmv.com.csv\", \"1070_wnep.com.csv\", \"686_military.com.csv\", \"1539_geocaching.com.csv\", \"641_cbsinteractive.com.csv\", \"200_washingtonpost.com.csv\", \"135_instagram.com.csv\", \"1360_thehill.com.csv\", \"1306_chasepaymentech.com.csv\", \"1470_steampowered.com.csv\", \"186_abcnews.com.csv\", \"1610_post-gazette.com.csv\", \"1708_foxsports.com.csv\", \"635_playstation.com.csv\", \"175_mlb.mlb.com.csv\", \"541_ifsa-butler.org.csv\", \"359_vikings.com.csv\", \"1259_fool.com.csv\", \"133_fortune.com.csv\", \"1300_bankofamerica.com.csv\", \"962_lynda.com.csv\", \"1106_allstate.com.csv\", \"1582_msn.com.csv\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "training_data = get_vector_for_policy(all_policies, training_files)\n",
    "t_data = {}\n",
    "for i in training_data:\n",
    "    if not np.isnan(get_paragraph_vector(i)).any():\n",
    "        t_data[i] = training_data[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "len(t_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def train_single_file(train_data):\n",
    "    x_data = []\n",
    "    for i in train_data.keys():\n",
    "        x_data.append(get_paragraph_vector(i))\n",
    "\n",
    "    y_data = list(train_data.values())\n",
    "    \n",
    "    \n",
    "    \n",
    "    scores = []\n",
    "    knns = []\n",
    "    for i in range(1, 10):\n",
    "        knn = KNeighborsClassifier(n_neighbors = i)\n",
    "        mean_score = get_k_fold_cross_validation(knn, x_data, y_data, 5)\n",
    "        scores.append(mean_score)\n",
    "        knns.append(knn)\n",
    "\n",
    "    index = np.argmax(scores)\n",
    "    \n",
    "    print(\"Grid search result:\")\n",
    "    print(scores[index])\n",
    "    print(scores)\n",
    "    return knns[index], x_data, y_data \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "knn_gscv, x_data, y_data = train_single_file(t_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "para_test_dict = {}\n",
    "\n",
    "for filename in all_policies.keys():\n",
    "    if filename not in training_files:\n",
    "        for key in all_policies[filename]:\n",
    "            para_test_dict[key] = CLASS_NUM[all_policies[filename][key]]\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "y_predict = []\n",
    "\n",
    "for i in para_test_dict.keys():\n",
    "\n",
    "    if np.isnan(get_paragraph_vector(i)).any():\n",
    "        y_predict.append(9)\n",
    "    else:\n",
    "        y_predict.append(knn_gscv.predict([get_paragraph_vector(i)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "y_knn_predict = []\n",
    "for i in y_predict:\n",
    "    if isinstance(i, int):\n",
    "        y_knn_predict.append(i)    \n",
    "    else:\n",
    "        y_knn_predict.append(i[0])\n",
    "        \n",
    "y_knn_predict\n",
    "len(y_knn_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "data = [x_data, y_data ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLPClassifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "def optimize_parameters(data, noOfFirstHiddenLayer, maxIter, learningRateInit):\n",
    "    nf = noOfFirstHiddenLayer\n",
    "    mi = maxIter\n",
    "    lr = learningRateInit\n",
    "    params = [[x, y, 10 ** z]\n",
    "              for x in range(nf[0], nf[1] + nf[2], nf[2])\n",
    "              for y in range(mi[0], mi[1] + mi[2], mi[2])\n",
    "              for z in range(int(np.log10(lr[0])),\n",
    "                             int(np.log10(lr[1])) + lr[2],\n",
    "                             lr[2])]\n",
    "    scores = []\n",
    "    mlps = []\n",
    "    for param in params:\n",
    "        mlp = MLPClassifier((param[0], 10), max_iter=param[1],\n",
    "                            learning_rate_init=param[2])\n",
    "        mean_score = get_k_fold_cross_validation(mlp, data[0], data[1], 2)\n",
    "\n",
    "\n",
    "        scores.append(mean_score)\n",
    "        mlps.append(mlp)\n",
    "\n",
    "    index = np.argmax(scores)\n",
    "\n",
    "    print(\"Grid search result:\")\n",
    "\n",
    "    print(params[index])\n",
    "    print(scores[index])\n",
    "    return mlps[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "mlp_model = optimize_parameters(data, [100,130, 10],[200, 400, 100], [0.001, 0.1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "y_m_predict = []\n",
    "\n",
    "for i in para_test_dict.keys():\n",
    "\n",
    "    if np.isnan(get_paragraph_vector(i)).any():\n",
    "        y_m_predict.append(9)\n",
    "    else:\n",
    "        y_m_predict.append(mlp_model.predict([get_paragraph_vector(i)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "y_mlp_predict = []\n",
    "for i in y_m_predict:\n",
    "    if isinstance(i, int):\n",
    "        y_mlp_predict.append(i)    \n",
    "    else:\n",
    "        y_mlp_predict.append(i[0])\n",
    "        \n",
    "\n",
    "len(y_mlp_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "clf = LDA()\n",
    "mean_score = get_k_fold_cross_validation(clf, x_data, y_data, 5)\n",
    "\n",
    "mean_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "y_ld_predict = []\n",
    "\n",
    "for i in para_test_dict.keys():\n",
    "\n",
    "    if np.isnan(get_paragraph_vector(i)).any():\n",
    "        y_ld_predict.append(9)\n",
    "    else:\n",
    "        y_ld_predict.append(clf.predict([get_paragraph_vector(i)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "y_lda_predict = []\n",
    "for i in y_ld_predict:\n",
    "    if isinstance(i, int):\n",
    "        y_lda_predict.append(i)    \n",
    "    else:\n",
    "        y_lda_predict.append(i[0])\n",
    "        \n",
    "\n",
    "len(y_lda_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **********"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def get_policy_vectors(filepath):\n",
    "\n",
    "    with open(filepath,'r') as f:\n",
    "\n",
    "        data = nlp(remove_punctuation(f.read()))\n",
    "        res = {}\n",
    "        for sentence in data.sents:\n",
    "            sentence_sim_vector = calculate_sentence_sim(sentence)\n",
    "            # if max(sentence_sim_vector) >= 0.65:\n",
    "            #     print(sentence)\n",
    "            res[sentence] = sentence_sim_vector\n",
    "\n",
    "    return res\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def get_policy_vectors_sents(filepath):\n",
    "\n",
    "    with open(filepath,'r') as f:\n",
    "\n",
    "        data = nlp(remove_punctuation(f.read()))\n",
    "        res = {}\n",
    "        for sentence in data.sents:\n",
    "\n",
    "            sentence_sim_vector = get_sentence_vector(nlp((str(sentence.text))))\n",
    "\n",
    "            if np.isnan(sentence_sim_vector).any():\n",
    "                res[sentence] = np.array([0.0]*300)\n",
    "                continue\n",
    "            print(type(sentence_sim_vector))\n",
    "            # if math.isnan(sentence_sim_vector):\n",
    "            #     continue\n",
    "            # if max(sentence_sim_vector) >= 0.65:\n",
    "            #     print(sentence)\n",
    "            res[sentence] = sentence_sim_vector\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# def get_all_policy_vectors(src_path):\n",
    "#     all_policies = {}\n",
    "#     for file in os.listdir(src_path):\n",
    "#         with open(src_path + '/'+ file, 'r') as f:\n",
    "#             all_policies[file] = get_all_policy_vectors(src_path + '/'+ file)\n",
    "#\n",
    "#     return all_policies\n",
    "\n",
    "\n",
    "\n",
    "similarity_array = get_policy_vectors('../data/notags_policies/33_nbcuniversal.txt')\n",
    "vector_array = get_policy_vectors_sents('../data/notags_policies/33_nbcuniversal.txt')\n",
    "\n",
    "class_array= {}\n",
    "for k,row in similarity_array.items():\n",
    "    if max(row) >= 0.65:\n",
    "        class_array[k] = row.index(max(row)) + 1\n",
    "    else:\n",
    "        class_array[k] = 0\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# CLASS_NUM = {'Data Retention' : 0,\n",
    "#  'Data Security' : 1,\n",
    "#  'Do Not Track' : 2,\n",
    "#  'First Party Collection/Use' : 3,\n",
    "#  'International and Specific Audiences' : 4,\n",
    "#  'Policy Change' : 5,\n",
    "#  'Third Party Sharing/Collection' : 6,\n",
    "#  'User Access, Edit and Deletion' : 7,\n",
    "#  'User Choice/Control' : 8,\n",
    "#  'Other' : 9}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results: Knearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "\n",
    "predicted = y_knn_predict\n",
    "y_test =list(para_test_dict.values())\n",
    "\n",
    "\n",
    "\n",
    "precision_knn, recall_knn, fscore_knn, fscore_knn = score(y_test, predicted)\n",
    "\n",
    "print('precision: {}'.format(precision_knn))\n",
    "print('recall: {}'.format(recall_knn))\n",
    "print('fscore: {}'.format(fscore_knn))\n",
    "print('support: {}'.format(fscore_knn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results: MLP Classifier 2 Hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "\n",
    "predicted = y_mlp_predict\n",
    "y_test =list(para_test_dict.values())\n",
    "\n",
    "\n",
    "\n",
    "precision_mlp, recall_mlp, fscore_mlp, support_mlp = score(y_test, predicted)\n",
    "\n",
    "print('precision: {}'.format(precision_mlp))\n",
    "print('recall: {}'.format(recall_mlp))\n",
    "print('fscore: {}'.format(fscore_mlp))\n",
    "print('support: {}'.format(support_mlp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results: MLP Classifier 2 Hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "\n",
    "predicted = y_lda_predict\n",
    "y_test =list(para_test_dict.values())\n",
    "\n",
    "\n",
    "\n",
    "precision_lda, recall_lda, fscore_lda, support_lda = score(y_test, predicted)\n",
    "\n",
    "print('precision: {}'.format(precision_lda))\n",
    "print('recall: {}'.format(recall_lda))\n",
    "print('fscore: {}'.format(fscore_lda))\n",
    "print('support: {}'.format(support_lda))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "p_knn = [ 0.4962963  ,0.68452381 ,\n",
    "0.66666667 ,\n",
    "0.75282309 ,\n",
    "0.36645963 ,\n",
    "0.75510204 ,\n",
    "0.71590909,\n",
    " 0.4375  ,\n",
    "0.46428571, \n",
    "0.47469459,\n",
    "]\n",
    "p_mlp = [ 0.49411765     ,\n",
    "0.78709677    ,\n",
    "0.6 ,\n",
    "0.7321238  ,\n",
    "0.66666667 ,\n",
    "0.64893617,\n",
    "0.71538462 ,\n",
    "0.55405405 ,\n",
    "0.57843137 ,\n",
    "0.60587002,\n",
    "]\n",
    "p_lda = [ 0.57037037 ,\n",
    "0.82014388 ,\n",
    "0.18181818 ,\n",
    "0.67572816 ,\n",
    "0.62686567 ,\n",
    "0.73076923,\n",
    "0.7652439  ,\n",
    "0.50649351,\n",
    "0.64423077 ,\n",
    "0.62962963,\n",
    "]\n",
    "plt.title(\"Precision by different models\")\n",
    "plt.scatter(range(0,10), p_knn, color='blue', label= 'Knn')\n",
    "plt.scatter(range(0,10), p_mlp, color='orange', label = 'MLP' )\n",
    "plt.scatter(range(0,10), p_lda, color='green', label=\"LDA\")\n",
    "\n",
    "plt.xticks([0,1, 2,3,4,5,6,7,8,9],('Data Retention','Data Security', 'Do Not Track', 'First Party Collection/Use', 'International and Specific Audiences',\n",
    "            'Policy Change','Third Party Sharing/Collection','User Access, Edit and Deletion','User Choice/Control','Other'), rotation=90)\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "plt.title(\"Fscore by different models\")\n",
    "plt.scatter(range(0,10), fscore_knn, color='blue', label= 'Knn')\n",
    "plt.scatter(range(0,10), fscore_mlp, color='orange', label = 'MLP' )\n",
    "plt.scatter(range(0,10), fscore_lda, color='green', label=\"LDA\")\n",
    "\n",
    "plt.xticks([0,1, 2,3,4,5,6,7,8,9],('Data Retention','Data Security', 'Do Not Track', 'First Party Collection/Use', 'International and Specific Audiences',\n",
    "            'Policy Change','Third Party Sharing/Collection','User Access, Edit and Deletion','User Choice/Control','Other'), rotation=90)\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def get_accuracy(y_true, y_pred):\n",
    "    intersect = 0\n",
    "    for i in range(len(y_pred)):\n",
    "        if y_pred[i] == y_true[i]:\n",
    "            intersect = intersect + 1\n",
    "    accuracy = intersect/len(y_pred)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "get_accuracy(y_test, y_knn_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "get_accuracy(y_test, y_mlp_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "get_accuracy(y_test, y_lda_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "res_data = {}\n",
    "with open('../data/notags_policies/train_policies/627_dairyqueen.txt') as f:\n",
    "    dairy_queen_content = f.read()\n",
    "    \n",
    "    doc = nlp(dairy_queen_content)\n",
    "    count = 0\n",
    "    for sent in doc.sents:\n",
    "        count += 1\n",
    "        res_data[sent.text] = get_paragraph_vector(sent.text)\n",
    "        \n",
    "# dairy_queen_content\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "predicted_res = {}\n",
    "for i in res_data:\n",
    "    if not np.isnan(res_data[i]).any():\n",
    "        predicted_res[i] = mlp_model.predict([res_data[i]])[0]\n",
    "    else:\n",
    "        predicted_res[i] = 9\n",
    "len(predicted_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "res = defaultdict(list)\n",
    "for key, val in sorted(predicted_res.items()):\n",
    "    res[val].append(key)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Data Retention\n",
    "res[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "#  'Data Security' : 1,\n",
    "res[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    " #'Do Not Track' : 2,\n",
    "res[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    " #'First Party Collection/Use' : 3,\n",
    "res[3]\n",
    "len(res[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "#  'International and Specific Audiences' : 4,\n",
    "res[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# 'Policy Change' : 5,\n",
    "res[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "#  'Third Party Sharing/Collection' : 6,\n",
    "res[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "#  'User Access, Edit and Deletion' : 7,\n",
    "res[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "#  'User Choice/Control' : 8,\n",
    "res[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "#  'Other'\n",
    "len(res[9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "plt.title(\"Dairy Queen Privacy Policy\")\n",
    "y = [len(res[i]) for i in range(10)]\n",
    "\n",
    "plt.bar(range(0,10), y)\n",
    "# plt.scatter(range(0,10), fscore_mlp, color='orange', label = 'MLP' )\n",
    "# plt.scatter(range(0,10), fscore_lda, color='green', label=\"LDA\")\n",
    "\n",
    "plt.xticks([0,1, 2,3,4,5,6,7,8,9],('Data Retention','Data Security', 'Do Not Track', 'First Party Collection/Use', 'International and Specific Audiences',\n",
    "            'Policy Change','Third Party Sharing/Collection','User Access, Edit and Deletion','User Choice/Control','Other'), rotation=90)\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "## Analysis of Summary Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "with open('../data/notags_policies/sample_train/dict0.49543508709194095.txt') as f:\n",
    "    content = json.loads(f.read())\n",
    "content.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "x_analysis_data = {}\n",
    "y_analysis_data = {}\n",
    "\n",
    "\n",
    "for key in content:\n",
    "    x_a_data = []\n",
    "    y_a_data = []\n",
    "    for i in content[key]:\n",
    "            \n",
    "        if not np.isnan(get_paragraph_vector(i)).any():\n",
    "            x_a_data.append(get_paragraph_vector(i))\n",
    "            y_a_data.append(content[key][i])\n",
    "            \n",
    "    x_analysis_data[key] = x_a_data\n",
    "    y_analysis_data[key] = y_a_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "all_analysis_mlp = {}\n",
    "for key in content:\n",
    "    all_analysis_mlp[key] = optimize_parameters([x_analysis_data[key],y_analysis_data[key]], [100,130, 10],[200, 400, 100], [0.001, 0.1, 1])\n",
    "    \n",
    "\n",
    "all_analysis_mlp\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "with open('../data/notags_policies/sample_train/dict0.4494910647887381.txt') as f:\n",
    "    test_content = json.loads(f.read())\n",
    "    \n",
    "    \n",
    "predicted_y_analysis ={} \n",
    "for key in test_content:\n",
    "    y_temp_predict = []\n",
    "    for i in test_content[key].keys():\n",
    "\n",
    "        if np.isnan(get_paragraph_vector(i)).any():\n",
    "            y_temp_predict.append(-1)\n",
    "        else:\n",
    "            y_temp_predict.append(all_analysis_mlp[key].predict([get_paragraph_vector(i)]))\n",
    "    \n",
    "    predicted_y_analysis[key] = y_temp_predict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "threat_predict = {}\n",
    "for key in CLASS_NUM:\n",
    "    y_temp_predict = []\n",
    "    temp = CLASS_NUM[key]\n",
    "    if np.isnan(get_paragraph_vector(' '.join(res[temp]))).any():\n",
    "        y_temp_predict.append(-1)\n",
    "    else:\n",
    "        \n",
    "        \n",
    "        y_temp_predict.append(all_analysis_mlp[key].predict([get_paragraph_vector(' '.join(res[temp]))]))\n",
    "    threat_predict[key] = y_temp_predict\n",
    "threat_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "plt.title(\"Dairy Queen Privacy Policy\")\n",
    "y = [len(res[i]) for i in range(10)]\n",
    "\n",
    "barlist = plt.bar(range(0,10), y)\n",
    "barlist[1].set_color('r')\n",
    "barlist[2].set_color('r')\n",
    "barlist[3].set_color('r')\n",
    "barlist[6].set_color('r')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# plt.scatter(range(0,10), fscore_mlp, color='orange', label = 'MLP' )\n",
    "# plt.scatter(range(0,10), fscore_lda, color='green', label=\"LDA\")\n",
    "\n",
    "plt.xticks([0,1, 2,3,4,5,6,7,8,9],('Data Retention','Data Security', 'Do Not Track', 'First Party Collection/Use', 'International and Specific Audiences',\n",
    "            'Policy Change','Third Party Sharing/Collection','User Access, Edit and Deletion','User Choice/Control','Other'), rotation=90)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "for key in test_content:\n",
    "    print(key, test_content[key].values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "predicted_y_a = {}\n",
    "for key in test_content:\n",
    "    temp =[]\n",
    "    for i in predicted_y_analysis[key]:\n",
    "        if isinstance(i, int):\n",
    "            temp.append(i)    \n",
    "        else:\n",
    "            temp.append(i[0])\n",
    "    predicted_y_a[key] = temp\n",
    "\n",
    "    \n",
    "predicted_y_a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "\n",
    "\n",
    "for key in test_content:\n",
    "    print(score(list(test_content[key].values()), predicted_y_a[key])[0])\n",
    "\n",
    "# test_content['First Party Collection/Use'].values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "for key in test_content:\n",
    "    print(key)\n",
    "    print(get_accuracy(list(test_content[key].values()), predicted_y_a[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "{'First Party Collection/Use': ['1', '1', '1', '1', '0'],\n",
    " 'Third Party Sharing/Collection': ['0', '1', '1', '0', '0'],\n",
    " 'User Choice/Control': ['1', '1', '0', '1', '1'],\n",
    " 'Data Security': ['1', '1', '1', '1', '0'],\n",
    " 'User Access, Edit and Deletion': ['1', '1', '1', '1', '1'],\n",
    " 'International and Specific Audiences': ['0', '0', '0', '0', '1'],\n",
    " 'Data Retention': ['0', '0', '0', '0', '0'],\n",
    " 'Policy Change': ['0', '0', '1', '0', '0'],\n",
    " 'Do Not Track': ['0', 1, '0', '0'],\n",
    " 'Other': ['0', '0', '1', '0', '0']}\n",
    "\n",
    "CLASS_NUM = {'Data Retention' : 0,\n",
    " 'Data Security' : 1,\n",
    " 'Do Not Track' : 2,\n",
    " 'First Party Collection/Use' : 3,\n",
    " 'International and Specific Audiences' : 4,\n",
    " 'Policy Change' : 5,\n",
    " 'Third Party Sharing/Collection' : 6,\n",
    " 'User Access, Edit and Deletion' : 7,\n",
    " 'User Choice/Control' : 8,\n",
    " 'Other' : 9}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
